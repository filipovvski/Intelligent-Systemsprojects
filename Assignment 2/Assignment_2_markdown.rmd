---
title: "Untitled"
author: "Mateusz Kowalewski Piotr Filipowicz"
date: "9 01 2020"
output:
  word_document: default
  html_document: default
  pdf_document: default
---
#Assigment 2


1.Cleaning 
```{r}
knitr::opts_chunk$set(echo = TRUE)
#libraries inclusion
library(tm)
library(dplyr)
library(ggplot2)
library(proxy)
library(wordcloud)
library(utf8)
library(decoder)
library(devtools)
library(openNLPmodels.en)
library(NLP)
library(openNLP)
library(stringr)
detach("package:ggplot2", unload=TRUE)





```


```{r}
setwd("c:/users/kowal/desktop/inteligent/R projects/P_2")

cleanFun <- function(htmlString) {
  return(gsub("<.*?>", "", htmlString))
}

unescape_unicode <- function(x){
  #single string only
  stopifnot(is.character(x) && length(x) == 1)
  
  #find matches
  m <- gregexpr("(\\\\)+u[0-9a-z]{4}", x, ignore.case = TRUE)
  
  if(m[[1]][1] > -1){
    #parse matches
    p <- vapply(regmatches(x, m)[[1]], function(txt){
      gsub("\\", "\\\\", parse(text=paste0('"', txt, '"'))[[1]], fixed = TRUE, useBytes = TRUE)
    }, character(1), USE.NAMES = FALSE)
    
    #substitute parsed into original
    regmatches(x, m) <- list(p)
  }
  
  x
}

unescape_unicode2 <- function(x){
  #single string only
  stopifnot(is.character(x) && length(x) == 1)
  
  #find matches
  m <- gregexpr("(\\\\)+u[0-9a-z]{4}", x, ignore.case = TRUE)
  
  if(m[[1]][1] > -1){
    #parse matches
    p <- vapply(regmatches(x, m)[[1]], function(txt){
      gsub("\\", "\\\\", parse(text=paste0('"', txt, '"'))[[1]], fixed = TRUE, useBytes = TRUE)
    }, character(1), USE.NAMES = FALSE)
    
    #substitute parsed into original
    regmatches(x, m) <- list(p)
  }
  
  x
}

train = read.table('train.tsv', header=T, sep="\t", colClasses=c("factor", "character"),quote = "", encoding="UTF-8" , fileEncoding = "UTF-8", dec=".")

for (x in 1:2220){
  train$text_a[x] <- unescape_unicode(train$text_a[x])
  train$text_a[x] <- unescape_unicode2(train$text_a[x])
}

train$text_a <- gsub('\\\\+x[0-9a-z]{2}', '', train$text_a)
train$text_a <- gsub("\\\\n"," ", train$text_a)
train$text_a <- cleanFun(train$text_a)
train$text_a <- gsub("http.*","", train$text_a)

texts = train %>% pull('text_a')
labels = train %>% pull('label')
corpus <- Corpus(VectorSource(texts))

corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords('english'))


conn = file("english.stop.txt", open="r")
mystopwords = readLines(conn)
close(conn)
mystopwords <- c(mystopwords, 'ż', 'ź', 'ć', 'ą', 'ę')

corpus <- tm_map(corpus, removeWords, mystopwords)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, stemDocument)
#test data

test = read.table('test.tsv', header=T, sep="\t", colClasses=c("factor", "character"),quote = "", encoding="UTF-8" , fileEncoding = "UTF-8", dec=".")
#train$text_a <- gsub(".*/","", train$text_a)


for (x in 1:987){
  train$text_a[x] <- unescape_unicode(train$text_a[x])
  train$text_a[x] <- unescape_unicode2(train$text_a[x])
}

test$text_a <- gsub('\\\\+x[0-9a-z]{2}', '', test$text_a)
test$text_a <- gsub("\\\\n"," ", test$text_a)
test$text_a <- cleanFun(test$text_a)
test$text_a <- gsub("http.*","", test$text_a)

texts_test = test %>% pull('text_a')
labels_test = test %>% pull('label')
corpus_test <- Corpus(VectorSource(texts_test))

corpus_test <- tm_map(corpus_test, content_transformer(tolower))
corpus_test <- tm_map(corpus_test, removePunctuation)
corpus_test <- tm_map(corpus_test, removeNumbers)
corpus_test <- tm_map(corpus_test, removeWords, stopwords('english'))
corpus_test<- tm_map(corpus_test,stemDocument)

conn = file("english.stop.txt", open="r")
mystopwords = readLines(conn)
close(conn)
mystopwords <- c(mystopwords, 'ż', 'ź', 'ć', 'ą', 'ę')

corpus_test <- tm_map(corpus_test, removeWords, mystopwords)
corpus_test <- tm_map(corpus_test, stripWhitespace)


```

2.Exploration

```{r}

all_corpus <- c(corpus, corpus_test)
tdm <- TermDocumentMatrix(corpus)
tdm
findFreqTerms(tdm, lowfreq=50)
tdm


termFrequency <- rowSums(as.matrix(tdm))
termFrequency <- subset(termFrequency, termFrequency >= 50)
qplot(seq(length (termFrequency)),sort(termFrequency), xlab = "index", ylab = "Freq")
termFrequency


```

Word that occurs the most is "dont". Apart from it there are some insults and commonly used words.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Convert the term-document matrix to a normal matrix and calculate word frequencies
mat <- as.matrix(tdm)
wordFreq <- sort(rowSums(mat), decreasing=TRUE)
grayLevels <- gray((wordFreq+10) / (max(wordFreq)+10))
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=50, random.order=F, colors=grayLevels)





```

```{r setup, include=FALSE}


tdm2 <- removeSparseTerms(tdm, sparse=0.7)
mat <- as.matrix(tdm2)
distMatrix <- dist(mat)

dtm <- DocumentTermMatrix(corpus, control = list(weighting=weightTfIdf))
mat <- as.matrix(dtm)

k <- 2
kmeansResult <- kmeans(mat, k)


for (i in 1:k) 
{
  s <- sort(kmeansResult$centers[i,], decreasing=T)
  cat(names(s)[1:10], "\n") 
  
}

```
```{r setup, include=FALSE}

dtm <- DocumentTermMatrix(corpus, control = list(weighting=weightTfIdf))
mat <- as.matrix(dtm)


k <- 4
kmeansResult <- kmeans(mat, k)

for (i in 1:k) 
{
  s <- sort(kmeansResult$centers[i,], decreasing=T)
  cat(names(s)[1:10], "\n")
  
}



```
```{r setup, include=FALSE}

dtm <- DocumentTermMatrix(corpus, control = list(weighting=weightTfIdf))
mat <- as.matrix(dtm)

# Cluster the documents using the kmeans method with the number of clusters set to 3 
k <- 8
kmeansResult <- kmeans(mat, k)

# Find the most popular words in every cluster
for (i in 1:k) 
{
  s <- sort(kmeansResult$centers[i,], decreasing=T)
  cat(names(s)[1:10], "\n")
  
}


```

```{r setup, include=FALSE}

dtm <- DocumentTermMatrix(corpus, control = list(weighting=weightTfIdf))
mat <- as.matrix(dtm)

# Cluster the documents using the kmeans method with the number of clusters set to 4 
k <- 16
kmeansResult <- kmeans(mat, k)

# Find the most popular words in every cluster
for (i in 1:k) 
{
  s <- sort(kmeansResult$centers[i,], decreasing=T)
  cat(names(s)[1:10], "\n")
  
}



```
```{r setup, include=FALSE}
text.pca <- prcomp(mat)
comp1 <- as.numeric(text.pca$x[,1])
comp2 <- as.numeric(text.pca$x[,2])

cos <- factor(kmeansResult$cluster)
qplot(comp1, comp2, colour= cos)


```
We projected the representation to two dimensions via PCA and visualized the cluster assignments(k=16). we can observe 2 main cluster, few smaller clusters and another few clusters made of words that are basically just random characters. 


```{r setup, include=FALSE}
qplot(comp1, comp2, colour= labels)
```
We can observe two clusters that are really close to each other and there is no nice border between them.
```{r setup, include=FALSE}
#POS tags
library(NLP)
library(openNLP)
library(qdapRegex)
library(ggplot2)
library(reshape2)
library(openNLPmodels.en)
library(openNLP)
library(dplyr)
library(stringr)
library(data.table)
detach("package:ggplot2", unload=TRUE)

# Construct a document-term matrix

##################################################################################

data.tfidf <- DocumentTermMatrix(corpus, control = list(weighting=weightTfIdf))
text.mat <- as.matrix(data.tfidf)

# Construct a data set
data <- cbind(text.mat)
training <- data
sub <- data[,c(1:200)]
sub


#test

data_test.tfidf <- DocumentTermMatrix(corpus_test, control = list(weighting=weightTfIdf))
text_test.mat <- as.matrix(data_test.tfidf)

# Construct a data set
data_test <- cbind(text_test.mat)
testing <- data_test[,c(1:200)]

colnames <- colnames(data, do.NULL = TRUE, prefix = "col")
colnames <- paste(colnames,collapse=" ")
colnames <- as.String(colnames)

s <- as.String(corpus)
s <- gsub('"', '', s)
s <- gsub(',', '', s)
s <- gsub(')', '', s)
s <- as.String(words)

sent_ann <- Maxent_Sent_Token_Annotator()
a1 <- annotate(s, sent_ann)


word_ann <- Maxent_Word_Token_Annotator()
a2 <- annotate(s, word_ann, a1)
a2w <- subset(a2, type == "word")

pos_ann <- Maxent_POS_Tag_Annotator()

a3 <- annotate(s, pos_ann, a2)
a3w <- subset(a3, type == "word")

# Extract token/POS pairs

tags <- vector()
for (i in 1:length(a3w$features))
  tags <- c(tags, a3w$features[[i]]$POS)

tokenPOS <- cbind(s[a3w], tags)
tokenPOS

```

3.Modeling
The train data contains 1630 non insults and 590 insults. Test data contains 725 non insults and 262 insults. The data is inbalanced, we have 3 times more non insults than insults. we decided to use SVM and neural networks model because those models are the most succesfull models in NLP.
```{r setup, include=FALSE}
library(class)
library(kernlab)
library(tidyverse)
library(caret)
library(e1071)
#install.packages('tidyverse')


classifier = ksvm(labels ~ .,
                  data = sub,
                  type = 'C-svc',
                  kernel = "rbfdot",
                  C = 10,
                  default = "automatic",
                  cross = 5)

pred <- predict(classifier, testing, type="response")

cm0 = table(labels_test, pred)
acc0 <- sum(diag(cm0)) / sum(cm0)
acc0

```


```{r setup, include=FALSE}
classifier = ksvm(labels ~ .,
                  data = sub,
                  type = 'C-svc',
                  kernel = "polydot",
                  default = "automatic",
                  cross = 5)

pred <- predict(classifier, testing, type="response")
cm1 = table(labels_test, pred)
acc1 <- sum(diag(cm1)) / sum(cm1)
acc1

```

```{r setup, include=FALSE}
classifier = ksvm(labels ~ .,
                  data = sub,
                  type = 'C-svc',
                  kernel = "vanilladot",
                  default = "automatic",
                  cross = 5)

pred <- predict(classifier, testing, type="response")

cm2 = table(labels_test, pred)
acc2<- sum(diag(cm2)) / sum(cm2)
acc2


```

```{r setup, include=FALSE}
classifier = ksvm(labels ~ .,
                  data = sub,
                  type = 'C-svc',
                  kernel = "tanhdot",
                  default = "automatic",
                  cross = 5)
pred <- predict(classifier, testing, type="response")

cm3 = table(labels_test, pred)
acc3<- sum(diag(cm3)) / sum(cm3)
acc3


```

```{r setup, include=FALSE}
classifier = ksvm(labels ~ .,
                  data = sub,
                  type = 'C-svc',
                  kernel = "laplacedot",
                  default = "automatic",
                  cross = 5)

pred <- predict(classifier, testing, type="response")

cm4 = table(labels_test, pred)
acc4<- sum(diag(cm4)) / sum(cm4)
acc4


```

```{r setup, include=FALSE}
classifier = ksvm(labels ~ .,
                  data = sub,
                  type = 'C-svc',
                  kernel = "besseldot",
                  default = "automatic",
                  cross = 5)

pred <- predict(classifier, testing, type="response")

cm5 = table(labels_test, pred)
acc5<- sum(diag(cm5)) / sum(cm5)
acc5

```
The classification performance of SVM is around 73% at best. The data is not clean enough to give performance. Also adding POS tags would probably gives better performance. Data set could also be little more blanced end bigger to gives better perfromance.


4.Understanding
```{r setup, include=FALSE}
control <- trainControl(method='repeatedcv',number=3, repeats=1)


rf.model <- train(x = data[,c(1:200)], 
                  y = labels , 
                  method='rf', 
                  metric='Accuracy', 
                  trControl=control)

importance.df.values <- rf.importances$importance
importance.df.names <- rownames(rf.importances$importance)
importance.rf.whole <- data.frame(score = importance.df.values,cnames = importance.df.names)
importance.rf.whole <- importance.rf.whole[order(importance.rf.whole$Overall, decreasing = T),]
feature.names.wrap<- importance.rf.whole$cnames[1:200]
ficzers.rf <- feature.names.wrap
ficzers.rf

# Re-evaluate the models performance for top n features 
ind <- match(ficzers.relief, colnames(data))
ficzers.relief <- cbind(data[,ind])

ind <- match(ficzers.rf, colnames(data))
ficzers.rf <- cbind(data[,ind])

classifier = ksvm(labels ~ .,
                  data = ficzers.rf,
                  type = 'C-svc',
                  kernel = "rbfdot",
                  C = 10,
                  default = "automatic",
                  cross = 5)

pred <- predict(classifier, testing[,c(1:200)], type="response")

cm_rf = table(labels_test, pred)
acc_rf <- sum(diag(cm_rf)) / sum(cm_rf)

classifier = ksvm(labels ~ .,
                  data = ficzers.relief,
                  type = 'C-svc',
                  kernel = "rbfdot",
                  C = 10,
                  default = "automatic",
                  cross = 5)

pred <- predict(classifier, testing[,c(1:200)], type="response")

cm_relief = table(labels_test, pred)
acc_relief <- sum(diag(cm_relief)) / sum(cm_relief)

acc_rf
acc_relief


```

```{r setup, include=FALSE}
intersect(feature.names.wrap, feature.names.relief)
paste0(length(intersect(feature.names.wrap[1:20], feature.names.relief[1:20]))/length(union(feature.names.wrap[1:20], feature.names.relief[1:20]))*100,"% overlap between top ranked features!")

#plot the jaccard score 
for (n in c(1:200))
{
 c[n] <- paste0(length(intersect(feature.names.wrap [1:n], names(relief.importances[1:n])))/
           length(union(feature.names.wrap [1:n], names(relief.importances[1:n])))*100)
}
n = c(1:200)
plot(n,c)
```
The jaccard score isn't evaluated on whole data set because the compilation of wrapper and filter method on whole data set would take too long.
At first the jaccard score is increasing but later it is decreasing due to larger amount of features.


